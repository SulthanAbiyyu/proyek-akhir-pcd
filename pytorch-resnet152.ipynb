{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Adam, get_linear_schedule_with_warmup, AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DATASET_PATH = Path('./cifake/')\n",
    "TRAIN_DATASET_PATH = ROOT_DATASET_PATH / 'train'\n",
    "TEST_DATASET_PATH = ROOT_DATASET_PATH / 'test'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNextImageProcessor {\n",
       "  \"crop_pct\": 0.875,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"ConvNextImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"microsoft/resnet-152\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = Normalize(mean=image_processor.image_mean,\n",
    "                      std=image_processor.image_std)\n",
    "if \"height\" in image_processor.size:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = image_processor.size.get(\"longest_edge\")\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(crop_size),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        Resize(size),\n",
    "        CenterCrop(crop_size),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(\n",
    "        image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "train_ds = datasets.ImageFolder(root=TRAIN_DATASET_PATH, transform=train_transforms)\n",
    "test_ds = datasets.ImageFolder(root=TEST_DATASET_PATH, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet152(pretrained=True)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "model.to(DEVICE)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = checkpoint.split(\"/\")[-1]\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"]\n",
    "                               for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "eval_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=len(train_loader) * EPOCHS * 0.1,\n",
    "    num_training_steps=len(train_loader) * EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "wandb.init(project=\"proyek-akhir-pcd\", name=\"resnet152-5epoch-pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for pixel_values, labels in loader:\n",
    "            pixel_values, labels = pixel_values.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(pixel_values)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return total_loss / len(loader), f1\n",
    "\n",
    "step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for pixel_values, labels in progress_bar:\n",
    "        pixel_values, labels = pixel_values.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # Forward and backward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Logging with WANDB\n",
    "        wandb.log({'Train Loss': loss.item()}, step=step)\n",
    "\n",
    "        # Every 200 steps, perform evaluation\n",
    "        if step % 200 == 0:\n",
    "            eval_loss, eval_f1 = evaluate(model, eval_loader)\n",
    "            wandb.log({'Eval Loss': eval_loss, 'Eval F1': eval_f1}, step=step)\n",
    "\n",
    "        # Every 1200 steps, save a checkpoint\n",
    "        if step % 1200 == 0:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, f\"checkpoint_{step}.pth\")\n",
    "\n",
    "        progress_bar.set_postfix(train_loss=loss.item())\n",
    "        step += 1\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (biyu)",
   "language": "python",
   "name": "biyu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
